#基于flume，kafka，sparkstreaming的日志分析demo
##一.概述


###Flume
Flume是Cloudera开发的一个分布式日志收集系统，具有高可靠、高可用、事务管理、失败重启等功能。数据处理速度快，在生产环境中已经得到了广泛的应用。


Flume的核心是agent。Agent是一个java进程，运行在日志收集端，通过agent接收日志，然后暂存起来，再发送到目的地。


Agent里面包含3个核心组件：source、channel、sink。
示意图如下：


- Source组件用于收集日志，可以处理各种类型各种格式的日志数据。在我们的demo中source的种类是exec，他是以运行linux命令的方式，去实时输出log中的数据


- Channel组件专用于临时存储source发过来的数据，channel的类型有memory、jdbc、file、自定义等。在我们的demo中，channel的类型是memorychannel，这种方式读写速度比较快，但是无法保证数据的完整性，适合处理数据较小的场合。


- Sink组件是用于把数据发送到目的地的组件，目的地包括hdfs、logger、avro、thrift、ipc、file、null、hbase、solr、自定义。
另外，在flume的整个数据传输过程中，流动的是event（由一行数据封装而成）。事务保证是在event级别。也正是这种事务管理的机制，支撑了flume的高可靠性。


具体配置，见环境搭建章节。

###Kafka
Apache Kafka是分布式发布-订阅消息系统。它最初由LinkedIn公司开发，之后成为Apache项目的一部分。


Kafka是一种快速、可扩展的、设计内在就是分布式的，分区的和可复制的提交日志服务。它主要用于处理活跃的流式数据。


架构图如下：

在kafka中，核心构建包括三部分：producer，broker，consumer

- Producer是消息生产者，向kafka cluster发送message的客户端，在我们的demo里，flume就是producer，kafka的数据来自于他。


- Broker是kafka cluster的节点。每个broker有唯一的编号。所有从producer push来的数据，都是存在这上面的。他的主要功能就是数据存储。


- Consumer是消息消费者，从kafka cluster拉取消息，进行消费的的客户端。在我们的demo里，spark就是consumer。注意consumer是主动从broker上取数据的，他的拉取速度取决于他的消费速度。


另外值得一提的broker的数据存储。相关概念：topic，partition，replication。

- topic：消息主题，可以在逻辑上把他理解为一个队列。producer生成的message，就是push到broker中的topic里，等待被消费的。实际上，topic是由多个partition组成的。Broker的数据存储方式，也是按partition来的。


- partition：分区，每个topic由多个partition组成，每个partition中存储部分topic的数据。partition的物理存储形式，是独立的文件夹。比如在producer端指定了一个topic的名字是test，那么这些partition对应的文件名，应该就是test_0,test_1,test_2(后面跟的是partition的编号)


- replication：副本，每个partition可以有自己的副本，为了容灾及提高可用性考虑，可以设置创建1到2个partition副本到集群中其他节点中存储。同一个partition有多个备份，但真正和producer，consumer交互，进行数据读写的是leader partition。这个leader partition是由zookeeper从众多partition中选举出来的，其他partition都是follower partition。可以在$KAFKA_HOME/config/server.properties中指定num.partitions，配置topic由几个partition组成。指定default.replication.factor，配置每个partition有几个备份。


> **数据push阶段：**
> 
> producer产生message，并且指定消息对应的topic。每条message有一个key值，同时加上producer指定的partition机制。可以确定该message被push到broker（kafka server）的哪个leader partition（由zookeeper选举）上。然后，各个follower partition将数据同步到自己的存储空间。
> 
> 
> **数据pull阶段：**
> 
> consumer会以pull的方式，从broker那里消费数据。具体讲，Consumer启动后会在Zookeeper上注册下consumergroup，consumerID，owers，offset等信息。其中owners记录consumer消费哪些topic，以及对应的partition。offset记录该consumer消费到哪里了，就是偏移量。consumer pull数据会根据zookeeper上保存的元数据信息，找到对应topic的offset偏移量，从那里开始消费数据。


###Spark Streaming
Spark streaming是对spark core api的扩展，用于流式数据的处理。

Spark streaming可以从多种数据源接收数据，利用内部提供的api进行数据处理，最后将数据存储在hdfs，数据库或者仪表盘中。其中streaming是通过dstream的创建，转换，output操作来处理数据的。


Spark streaming内部会将dstream按照某个时间间隔划分成一个个mini batch，每个mini batch就是一个rdd，然后通过dstream的transformation操作和output操作形成的dstreamgraph，生成当前rdd的rdd DAG，交由spark engine进行rdd的计算。


> **数据流转过程:**
> 
> 
> **Flume**(producer)监控日志文件，当这个日志文件中有新的数据后，通过agent source的exec方式，将数据push到**kafka** broker对应的partition上，等待数据被消费。
> **Spark streaming**(consumer)根据zookeeper上存储的元数据信息（consumer消费哪些partition以及具体消费的偏移量offset），从kafka broker上对应的partition那里pull数据，进行数据的处理。
> 
##二.环境搭建



**1.下载Flume和Kafka集成的插件，下载地址：**


https://github.com/beyondj2ee/flumeng-kafka- plugin。


将package目录中的flumeng-kafka-plugin.jar拷贝到Flume安装目录的lib目录下




**2.将Kakfa安装目录libs目录下的如下jar包拷贝到Flume安装目录的lib目录下**


kafka_2.10-0.8.1.1.jar


scala-library-2.10.1.jar

metrics-core-2.2.0.jar



**3.添加flume的agent的配置文件（kafka_log.conf）**
	producer.sources = s   
	producer.channels = c
	producer.sinks = r
	#source section
	producer.sources.s.type = exec
	producer.sources.s.command = tail -f /opt/xcdong/spark-1.4.1-2/spark-1.4.1-bin-
	hadoop2.6/app_logs/app-20160219012631-0000.inprogress
	producer.sources.s.channels = c
	# Each sink's type must be defined
	producer.sinks.r.type = org.apache.flume.plugins.KafkaSink
	producer.sinks.r.metadata.broker.list=127.0.0.1:9092
	producer.sinks.r.partition.key=0
	producer.sinks.r.partitioner.class=org.apache.flume.plugins.SinglePartition
	producer.sinks.r.serializer.class=kafka.serializer.StringEncoder
	producer.sinks.r.request.required.acks=0
	producer.sinks.r.max.message.size=1000000
	producer.sinks.r.producer.type=sync
	producer.sinks.r.custom.encoding=UTF-8
	producer.sinks.r.custom.topic.name=test13
	#Specify the channel the sink and source should use
	producer.sinks.r.channel = c
	producer.sources.s.channel = c
	# Each channel's type is defined.
	producer.channels.c.type = memory
	producer.channels.c.capacity = 1000


	注：producer.sources.s.type = exec
	指定source的种类是exec，通过运行linux命令的方式来持续的输入最新的数据。
	producer.sinks.r.type = org.apache.flume.plugins.KafkaSink
	指定agent的sink是kafka
	producer.sinks.r.metadata.broker.list=127.0.0.1:9092
	指定kafka的broker list，即将来flume会把数据push到哪里
	producer.sinks.r.partitioner.class=org.apache.flume.plugins.SinglePartition
	指定flume（producer）端的partition机制，producer会根据这个类中实现的方法，将生成的message push到kafka broker上指定的partition中。
	producer.sinks.r.serializer.class=kafka.serializer.StringEncoder
	指定消息序列化类
	producer.sinks.r.request.required.acks=0
	默认就是这样的，producer不会等待broker发送ack。
	若这个配置设置为1，表示producer会等待broker发送ack，而broker端会在leader partition接收到数据之后，发送ack。
	若这个配置设置为2，表示producer会等待broker发送ack，而broker端会在follower partition同步消息成功后，发送ack。
	producer.sinks.r.producer.type=sync
	Sync是默认配置，表示producer消息发送的模式是同步的，就是producer产生message，就push到broker的partition中。
	要是配置成async，异步。Producer产生的message，会先存在本地buffer中，适时批量发送。
	producer.sinks.r.custom.encoding=UTF-8
	指定编码方式
	producer.sinks.r.custom.topic.name=test13
	指定topic名称
	producer.channels.c.type = memory
	producer.channels.c.capacity = 1000
	指定channel类型，为memeorychannel，以及memory的大小

**4.启动kafka**

zkServer.sh start（起zookeeper）


bin/kafka-server-start.sh config/server.properties &


注：kafka不需要做特殊配置，用server.properties里的特殊配置就可以了。主要是向zookeeper注册broker，以及broker上的topic和partition。


截图如下：	


**5.启动flume的agent（开始收集日志信息）**

bin/flume-ng agent --conf conf --conf-file kafka_log.conf --name producer -Dflume.root.logger=INFO,console


注：


--conf指定了flume的配置目录

--name producer 指定agent的名称

--conf-file 指定agent的配置文件

截图如下：

**6.在kafka中接收对应的topic消息**


bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test24--from-beginning &

注： --zookeeper指定zookeeper的地址，kafka需要向zookeeper注册broker上的topic和partition信息。

截图如下：


**7.使用streaming接受从flume，kafka那里读出的log**


在spark shell下运行程序demo


pyspark --master spark://sparksl05.eng.platformlab.ibm.com:13980 --jars /opt/xcdong/spark-1.4.1-bin-hadoop2.6/lib/spark-examples-1.4.1-hadoop2.6.0.jar

开启brokeh server（用于显示数据处理后的图像）


bokeh serve --host 9.21.62.237:5006 --host localhost:5006


http://9.21.62.237:5006/?bokeh-session-id=hehe 


运行结果截图：


###该demo，实现的功能是：
分析从flume,kafka那里传过来的日志，实时地计算出当前集群各个时间段内的task总数，图1显示的是运行SparkPi 100 生成的100个task，各个时间段内的发布情况。第二幅图中显示的是运行SparkPi 10000生成的10000个task。

##三. spark程序demo
    from __future__ import print_function
    import os
    import sys
    import json
    import time
    import math 
    from pyspark import SparkContext
    from pyspark.streaming import StreamingContext
    from pyspark.streaming.kafka import KafkaUtils
    from pyspark.sql import SQLContext, Row
    from threading import Timer  
    from operator import concat
    from bokeh.plotting import figure, output_file, show
    import numpy as np
    from numpy import pi
    from bokeh.client import push_session
    from bokeh.driving import cosine
    from bokeh.plotting import figure, curdoc
    from time import *
    import random
    
    //demo中使用spark streaming操作了sparksql，需要使用单例模式获取一个sqlcontext来进行sql操作。
    def getSqlContextInstance(sparkContext):
    if ('sqlContextSingletonInstance' not in globals()):
    globals()['sqlContextSingletonInstance'] = SQLContext(sparkContext)
    return globals()['sqlContextSingletonInstance']
    
    //获取日志中各个时间段内的task总数
    def process(time, rdd):
    global DATA
    global KEYS
    if (rdd.count() != 0):
    print("--------------------------time is %s" % time)
    try:
    temp = rdd.keys().count()
    if KEYS != temp:
    KEYS = temp
    print(KEYS)
    value = rdd.mapValues(lambda x: list(x)).reduceByKey(concat).collect()
    sqlContext = getSqlContextInstance(rdd.context)
    for (table_name, table_set) in value:
    registerDBTable(table_name, table_set, sqlContext)
    minStartTime =  sqlContext.sql("\
     SELECT min(ts.TaskInfo.LaunchTime) From \
     SparkListenerTaskStart ts \
    ").collect()[0][0]
    print("----minStartTime is %s" % strftime("%y-%m-%d %H:%M:%S",   
     localtime(float((minStartTime)/1000))))
    maxFinishTime = sqlContext.sql("\
     SELECT max(te.TaskInfo.FinishTime) From \
     SparkListenerTaskEnd te \
     ").collect()[0][0] 
    print("----maxFinishTime is %s" % strftime("%y-%m-%d %H:%M:%S", 
     localtime(float((maxFinishTime)/1000))))
    list_length = int((maxFinishTime - minStartTime) / 1000) + 5
    print(list_length)
    getInfo = sqlContext.sql(" \
      SELECT ts.TaskInfo.TaskID as taskId, ts.TaskInfo.ExecutorID as 
      executorId,ts.TaskInfo.Host as host, \
       ((ts.TaskInfo.LaunchTime - %s) / 1000) as startTime, \
      ((te.TaskInfo.FinishTime - %s) / 1000) as finishTime FROM \
      SparkListenerTaskStart ts LEFT OUTER JOIN SparkListenerTaskEnd
      te ON ts.TaskInfo.TaskID = te.TaskInfo.TaskID AND \
      ts.TaskInfo.ExecutorID = \
      te.TaskInfo.ExecutorID AND ts.TaskInfo.Host = te.TaskInfo.Host \
      " % (minStartTime, minStartTime))
    getInfo.registerTempTable("taskInfo")
    _taskInfo = sc.parallelize([i for i in range(list_length)]).map(lambda i: (i, 0))
    taskInfo = getInfo.select("startTime", "finishTime").
      where("finishTime IS NOT NULL").
      map(lambda task: getFloor(task.startTime, task.finishTime)).
      flatMap(lambda task: getTaskNumber(task[0],task[1])).
      map(lambda e: (e, 1)).reduceByKey(lambda a, b: a+b).
      rightOuterJoin(_taskInfo).sortByKey().
      map(lambda i: convertNone(i))
    DATA = taskInfo.collect()
    print("-----------------in process and the DATA is %s" % DATA)
    print("------------------end")  
    except:
    pass
    
    def getEvent(item):
      json_format = json.loads(item)
      key = json_format['Event']
      value = json.dumps(json_format).replace(' ', '')
      return (key, value)
    
    def registerDBTable(table_name, table_set, sqlContext):
      lineRdd = sc.parallelize(table_set)
      sqlContext.jsonRDD(lineRdd).registerTempTable(table_name)
      return
    
    def getFloor(startTime, finishTime):  
    return (int(startTime), int(finishTime))
    
    def getTaskNumber(startTime, finishTime):
    n = startTime 
    list = []
    while n <= finishTime:
    list.append(n)
    n = n + 1
    return list
    
    def getText(x):
     return str(x)
    
    def getResult(x, len):
    list = sorted(x)
    result = [0]*(len)
    for i in list:
    result[int(i[0])] = result[int(i[0])] + 1 
    return result
    
    def convertNone(i):
    if (i[1][0] == None):
    return 0
    else:
    return i[1][0]
    
    ssc = StreamingContext(sc, 2)//生成streaming context
    DATA = "111"
    DATA_OLD = DATA
    KEYS = ""
    
    topic = "test9"
    zkQuorum = "localhost:2181"
    
    lines = KafkaUtils.createStream(ssc, zkQuorum, "spark-streaming-consumer", {topic: 1}).
    map(lambda x: x[1])//通过spark提供的kafkautil创建inputDstream，从kafka那里接收数据
    words = lines.map(lambda line: getEvent(line)).groupByKey()//dstream的transformation操作
    windowDStream = words.window(86400, 2)//window操作，每两秒计算一次24小时内的数据
    windowDStream.foreachRDD(process)//dstream的output操作，开始真正的job的调度
    
    ssc.start()
    
    //以下是将处理之后的数据，利用bokeh提供的画图api画出来
    p = figure(title="statistics of cluster", x_axis_label='time', y_axis_label='taskCount')
    x = np.linspace(0, 4*pi, 80)
    y = np.sin(x)
    r1 = p.line(x, y, color="red", line_width=2)
    r = p.text(x=[], y=[], text=[], text_font_size="8pt", text_baseline="middle", text_align="center")
    # open a session to keep our local document in sync with server
    session = push_session(curdoc(), "hehe")
    print(session.id)
    
    @cosine(w=0.03)
    def update(step):
    global DATA
    global DATA_OLD
    if DATA != DATA_OLD:
    print("hehe")
    print(DATA)
    r1.data_source.data["x"] = np.arange(len(DATA))
    r1.data_source.data["y"] = DATA
    r.data_source.data['x'] = np.arange(len(DATA))
    r.data_source.data['y'] = DATA
    r.data_source.data['text'] = map(getText, DATA)
    DATA_OLD = DATA
    
    curdoc().add_periodic_callback(update, 100)
    session.show()
    session.loop_until_closed()
